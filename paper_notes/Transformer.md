## [All papers](../papers.md)
## [Attention is all you need, Transformer paper](https://arxiv.org/pdf/1706.03762.pdf)
1. abstract:
    1. Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.
    2. parallelizable and requiring significantly less time to train.
    3. WMT 2014 English-to-German translation: 28.4 BLEU(2 over the existing best results)
    4. WMT 2014 English-to-French translation: 41.8 BLEU, 3.5 days on eight GPUs.
    5. generalize well by appling on English constituency parsing both with large and limited training data. 
2. introduction
3. architecture 
4. conclusion
