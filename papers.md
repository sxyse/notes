# Notes for papers
* [NLP](#NLP)
  * [embedding](#embedding)
  * [seq2seq](#seq2seq)
* [ML](#ML)
* [paper notes](#paper-notes)
## NLP
### embedding
* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
* [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
### seq2seq
* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
* [Neural Machine Translation By Jointly Learning to Align and Translation, Original sequence-to-sequence + attention paper](https://arxiv.org/pdf/1409.0473.pdf)
* [Attention is all you need, Transformer paper](https://arxiv.org/pdf/1706.03762.pdf), **[notes](paper_notes/Transformer.md)**
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
* [A structured self-attentive sentence embedding](https://arxiv.org/pdf/1703.03130.pdf)
* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)
* [A Neural Conversational Model](https://arxiv.org/pdf/1506.05869.pdf), **[notes](paper_notes/A_Neural_Conversational_Model.md)**
## ML
* [PAMI papers](https://www.computer.org/csdl/trans/tp/index.html)
## paper notes
