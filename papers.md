# Notes for papers
* [NLP](#NLP)
  * [seq2seq](#seq2seq)
* [ML](#ML)
  
## NLP
### seq2seq
* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
* [Neural Machine Translation By Jointly Learning to Align and Translation, Original sequence-to-sequence + attention paper](https://arxiv.org/pdf/1409.0473.pdf)
* [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
* [A structured self-attentive sentence embedding](https://arxiv.org/pdf/1703.03130.pdf)
* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)
* [A Neural Conversational Model](https://arxiv.org/pdf/1506.05869.pdf)

## ML
* [PAMI papers](https://www.computer.org/csdl/trans/tp/index.html)
